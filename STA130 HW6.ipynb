{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3\n",
    "\n",
    "#The theoretical simple linear regression model, has fixed values of beta0 and beta1, as the intercept and slope \n",
    "#of the model, and these don't change. Hence, there is exactly only 1 unique line, corresponding to the theoretical\n",
    "#model. However, within the theoretical model, there is an additional error term epsilon_i, which is a random variable\n",
    "#with normal distribution with mean 0, and variance sigma^2. Note that, this error term is NOT plotted for the\n",
    "#theoretical line, only y = beta0 + beta1 * x is plotted. This error term accounts for deviation \n",
    "#of real world data from the theoretical line, where those deviations are not explained by the theoretical line.\n",
    "\n",
    "#The fitted line, is attained by first taking some random sample from the theoretical model, which includes the \n",
    "#error term. We would first take some number of x values, saved to a list, possibly done using some random sampling \n",
    "#methods, or non-random sampling methods such as linspace in python. Then, plug in each value of x into the\n",
    "#theoretical equation y = beta0 + beta1 * x to get the corresponding theoretical y values. Then, for each i-th\n",
    "#data point, we randomly sample from the normal distribution with mean 0 and variance sigma^2, to attain a \n",
    "#different error term epsilon_i for each theoretical value of y_i. We add the epsilon_i onto the theoretical value\n",
    "#of y_i, to get the \"real world sampled\" value of y_i. These y_i values mostly, should be scattered around the\n",
    "#theoretical line from the first part, but almost never lie precisely on the theoretical line, as the probability\n",
    "#of the error being precisely 0 is 0. The spread of the sampled values y_i, should have the same spread around \n",
    "#the theoretical line, independent of the value of x_i, as each sampled y_i, is attained from adding the \n",
    "#error term with the same variance sigma^2, onto the theoretical value of y_i.\n",
    "\n",
    "#After we have attained these \"real world sampled\" values of y_i, we can pretend that we didn't know the \n",
    "#theoretical values of beta0, and beta1, and try to \"backguess\" these values from the \"real world sampled\" values\n",
    "#of y_i, along with their corresponding x_i values. Now, the procedure in which the \"backguess beta_i from\n",
    "#the real world sampled values\" is quite complicated, but essentially, through some difficult math, we are able\n",
    "#to \"draw\" a new line that \"most optimally fits\" the \"real world sampled values\". Notice that the newly drawn\n",
    "#line would depend on the \"real worled sampled\" values y_i, and their corresponding x_i. However, since \n",
    "#these real world sampled values, include the error epsilon_i, it means that if we repeat this procedure a 2nd\n",
    "#time, and attain a new set of x_i and real world y_i, we would get a new line drawn, which presumably would be\n",
    "#very close to the 1st line drawn, but most likely not exactly the same. Similarly, repeating the procedure\n",
    "#for a 3rd time, a 4th time, etc. Hence, the fitted line is in fact a random line which depends on the\n",
    "#values of x_i, and the \"real world sampled\" values of y_i, and each time we get a new set of samples, a new\n",
    "#line will be drawn. Since all the samples are taken from the theoretical sample, we would expect that\n",
    "#all of these newly drawn lines would be quite close to the theoretical line, but not exactly the same, due to\n",
    "#each time, we get a different error epsilon_i value, which slightly causes the drawn line to deviate away\n",
    "#from the theoretical line.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ChatGPT summary: \n",
    "#This explanation does an excellent job of distinguishing between the theoretical line and the fitted line in\n",
    "#Simple Linear Regression, emphasizing the role of random sampling variation in creating a set of\n",
    "#\"real world sampled\" values and how this variation impacts the fitted line.\n",
    "\n",
    "#Summary of Key Points\n",
    "#Theoretical Model and Line:\n",
    "#Generation of \"Real World Sampled\" Data:\n",
    "#Fitting the Line to Real-World Data:\n",
    "\n",
    "#Purpose of Comparing the Two Lines\n",
    "#The comparison between the theoretical and fitted lines demonstrates how:\n",
    "\n",
    "#Theoretical Line: Represents the ideal relationship in a perfect world without any error or randomness.\n",
    "#Fitted Line: Represents our best estimate of the relationship, based on observed data that include random noise, which leads to slight variations with each resampling.\n",
    "\n",
    "#By visualizing both lines together, we gain insights into:\n",
    "\n",
    "#Sampling Variability: How different samples can yield slightly different fitted lines due to the randomness in each sample.\n",
    "#Estimation Consistency: How repeated fitted lines are generally close to the theoretical line, showing that our estimation process is consistent, even if not exact.\n",
    "#This approach helps us understand the robustness of our linear regression method in estimating an underlying relationship despite the influence of random error.\n",
    "\n",
    "\n",
    "#link: https://chatgpt.com/share/672d03a2-0d7c-800a-9881-4716a003e0d1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2110b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4:\n",
    "\n",
    "#After we have a data set of {(x_i, y_i)}, either simulated or attained otherwise somehow,\n",
    "#we perform the linear regression, through some complicated calculations,\n",
    "#to get our best \"guess\" on the slope and y-intercept in the theoretical model y = beta0 + beta1 * x, and\n",
    "#draw the \"line of best fit\", with these guesses on beta0 and beta1. \n",
    "#These best guesses are stored in the fitted_model.params variable, written out as beta0_hat, beta1_hat.\n",
    "#Using these guessed values of beta0 and beta1, we can now plug in any value of x into the equation\n",
    "#y = beta0_hat + beta1_hat * x, and attain the \"predicted value of y\", which is written as y_hat.\n",
    "#This y_hat is stored in the variable fitted_model.fittedvalues.\n",
    "#Although, this variable only stores the values of the predicted value of y: y_hat, \n",
    "#for the original values of x_i in the data, we could of course plug in any value of x into the above equation\n",
    "#to get the predicted value of y.\n",
    "#Since beta0_hat and beta1_hat is only a guess to the true values of beta0 and beta1, the value of y_hat\n",
    "#attained from plugging in x, is only a guess on the value of y (given the value of x), and is almost\n",
    "#never going to equal the true theoretical value for y = beta0 + beta1 * x (given the value of x). However,\n",
    "#if our data set is good enough, (such as large enough, and representative of the population, etc),\n",
    "#our fitted line would be close enough to the theoretical model, so our value of y_hat would be close\n",
    "#to the theoretical value of y.\n",
    "\n",
    "#ChatGPT summary:\n",
    "#This explanation provides a clear, step-by-step understanding of how fitted_model.fittedvalues are derived \n",
    "#based on the estimated parameters in fitted_model.params. Here’s a summary of the key points made, which\n",
    "#accurately describe the process of deriving y_hat (predicted values) from the fitted model:\n",
    "\n",
    "#Data Collection and Model Fitting:\n",
    "#Estimating Coefficients:\n",
    "#Predicting Y Values (Calculating y_hat):\n",
    "#Generalization to Any x:\n",
    "#Approximation and Accuracy of Predictions:\n",
    "\n",
    "#In Summary\n",
    "#The fitted_model.fittedvalues represents the predicted Y values derived by plugging the original \n",
    "# x values into the fitted regression equation with the estimated parameters beta0\n",
    "# and beta1. These values approximate the true Y values from the theoretical model, with their accuracy\n",
    "#depending on the quality and representativeness of the dataset.\n",
    "\n",
    "#Link: https://chatgpt.com/share/672d1562-025c-800a-a6bd-58eb7e34ab9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8e8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9:\n",
    "\n",
    "#The null hypothesis is: beta1 = 0\n",
    "#This means that there is no linear relationship between duration and waiting time.\n",
    "#The alternative hypothesis is: beta1 \\neq 0\n",
    "\n",
    "#When we restric the data set to only have short waiting times: less than 62,\n",
    "#We get a p-value of 0.238, which would not allow us to reject the null hypothesis.\n",
    "\n",
    "#When we restric the data set to only have short waiting times: less than 64,\n",
    "#We get a p-value of 0.036, which would allow us to reject the null hypothesis.\n",
    "\n",
    "#When we restric the data set to only have short waiting times: less than 66,\n",
    "#We get a p-value of 0.00036, which would not allow us to reject the null hypothesis.\n",
    "\n",
    "#Notice that as we increase the size of the data set, by allowing more and more data values\n",
    "#(first less than 62, then less than 64, then less than 66)\n",
    "#The p-value decreases. Especially when we use less than 66, where we have the most amount of data between the \n",
    "#three tries, we get a p-value that is very low, indicating there is a very strong linear relationship \n",
    "#between duration and waiting time.\n",
    "\n",
    "#This indicates that for waiting times of less than 62, the x and y values are fairly random. However, as\n",
    "#we use more values, especially when values between 64 and 66 are included, a significant linear relationship\n",
    "#emerges.\n",
    "\n",
    "#ChatGPT summary:\n",
    "#Your summary for Q9 is very well-structured and effectively explains the results from the hypothesis tests\n",
    "#on subsets of waiting times. Here’s a polished version that maintains your observations while enhancing\n",
    "#clarity and flow:\n",
    "\n",
    "#As we expand the dataset by increasing the waiting time limit (from less than 62 to less than 66), the p-value \n",
    "#steadily decreases. This pattern suggests that as we include more data points, especially those with waiting \n",
    "#times between 64 and 66 minutes, the linear relationship between waiting time and eruption duration becomes \n",
    "#more evident and statistically significant.\n",
    "#For waiting times below 62 minutes, the association between waiting time and eruption duration appears weak, \n",
    "#suggesting a relatively random relationship within this narrow window. However, as we expand to include waiting\n",
    "#times up to 66 minutes, a significant linear trend emerges, highlighting that a stronger relationship is indeed \n",
    "#present within the full range of short waiting times.\n",
    "\n",
    "#This conclusion effectively communicates how the strength of the linear relationship varies with the subset of waiting times considered. Great work on analyzing and explaining these results!\n",
    "\n",
    "\n",
    "#Link: https://chatgpt.com/share/672d1b42-d6ec-800a-87b4-43a4e298de80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76da1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q11:\n",
    "\n",
    "#The data \"arbitrarily\" splits the data into 2 groups of wait time \"<68\" as \"short\", and \">=68\" as \"long\". \n",
    "#I will not question why the specific number 68 is chosen to be the splitting value, and proceed with the analysis.\n",
    "\n",
    "#In this model, we have erased the continuous data x = wait time, and grouped the data into a categorical \n",
    "#variable, with 2 values: short and long. \n",
    "#Since we have chosen the indicator function to be on \"long\", it means that when k_i is \"short\", the indicator\n",
    "#evaluates to 0. Hence, the value beta_intercept, is essentially like an \"average/expected\" value of duration y,\n",
    "#when the input data x (turned into -> k), is short. This model essentially ignores any effect of the \n",
    "#continuous/numerical value of x, on the duration y, and assumes that all theoretical values of y,\n",
    "#would be a single value: beta_intercept, and any deviation from this \"average\" is \"explained\" by the error epsilon.\n",
    "\n",
    "#Similarly, the same is true when k_i is long, the value (beta_intercept + 1 * beta_contrast), is essentially\n",
    "#like an \"average/expected\" value of duration y, when the input data x (turned into -> k), is short. Any \n",
    "#deviation from this \"average\" is \"explained\" by the error epsilon.\n",
    "\n",
    "#Now, this model of course does not make sense, as we can not simply just throw any possible deviation from\n",
    "#the \"average\" to be error epsilon, as the numerical value of x, does (or should) have an effect on the value y. \n",
    "#In fact, assuming that all values of x below 68, would have a theoretical single value beta_intercept for y,\n",
    "#and all values of x above and equal to 68, would have a theoretical single value \n",
    "#beta_intercept + beta_contrast for y, is not a legitamate assumption.\n",
    "\n",
    "#The fact that the p value is extremely small, around 1.6e^-121, which is much smaller than 0.05,\n",
    "#would allow use to reject the null hypothesis being: there are no difference between the 2 groups.\n",
    "#We conclude that the alternative hypothesis is correct, however, since the model is so weak, we are only\n",
    "#able to say: there is \"some\" difference between the 2 groups, which is a very weak statement to say.\n",
    "\n",
    "#In contrast, to the previous 3 models, they all use x as a continuous variable, and try to predict y.\n",
    "#This gives us much more information about how increasing the value of x by 1 unit, would affect the value of y:\n",
    "#which is the value of slope beta1. This makes much more sense, instead of assuming all values of x (below 68)\n",
    "#would have the same theoretical y value: beta_intercept, and all values of x (above 68) would have the same\n",
    "#theoretical y value: beta_interecpt + beta_contrast.\n",
    "\n",
    "#Indeed, the 3 models differ slightly.\n",
    "#The 1st model uses the entire data set, which gives us a single intercept and a single slope.\n",
    "#This is a good starting point, as it would at least gives us the confirmation that there is some relationship\n",
    "#between x and y (when p < 0.05). However, it fails take into account of some possible difference in \n",
    "#the relationship, when x is small or when x is large.\n",
    "#The 2nd and 3rd model fits the data separately, split into 2 groups of short wait and long wait.\n",
    "#We would then able to get 2 sets of intercept and slope, for the 2 groups. This essentially means that \n",
    "#we are drawing 2 different lines on the scatter plot, one line for data points x<68, and another line for\n",
    "#data points x>= 68. This could potentially be a better fit, than drawing a single line using the 1st model.\n",
    "\n",
    "#But regardless, all 3 models are much better at giving information, than the indicator model above, as that\n",
    "#model fails to take into account of any affect of x on the value of y.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ChatGPT Summary:\n",
    "#Your analysis for Q11 is well-articulated and highlights the core differences between the indicator variable\n",
    "#model and continuous linear regression models effectively. Here's a polished version of your explanation that\n",
    "#organizes the insights while maintaining your thorough breakdown:\n",
    "\n",
    "#Q11 Analysis: Indicator Model vs. Continuous Linear Regression Models\n",
    "#The dataset divides wait times into two categories: “short” for wait times < 68 minutes and “long” for wait \n",
    "#times ≥ 68 minutes. Although the choice of 68 as the threshold may seem arbitrary, we’ll proceed with this \n",
    "#division for analysis.\n",
    "\n",
    "#In this indicator-based model, we simplify the continuous variable x (waiting time) into a categorical variable \n",
    "#k with two levels: \"short\" and \"long\". This transforms the regression into a comparison of mean eruption\n",
    "#durations between these two categories. Since we defined the indicator function based on \"long,\" the model\n",
    "#interprets:\n",
    "\n",
    "#Interpretation of the Indicator Model\n",
    "#This model disregards any continuous influence of the actual waiting time on eruption duration. It assumes\n",
    "#that within each category (short or long), eruption durations are roughly constant around their respective \n",
    "#averages, and any deviations are treated as random error ϵ. Such a model oversimplifies by discarding valuable\n",
    "#information about how specific waiting times may influence eruption durations.\n",
    "\n",
    "#Despite this, the model's extremely low p-value (~1.6e-121) enables us to reject the null hypothesis of\n",
    "#\"no difference between groups,\" indicating a statistically significant difference in eruption durations \n",
    "#between short and long wait times. However, since the model is very simplistic, the conclusion it provides\n",
    "#is weak—essentially only confirming that some difference exists without detailing how waiting time impacts\n",
    "#duration within each group.\n",
    "\n",
    "#Comparison with the Previous Continuous Models\n",
    "#In contrast, the earlier models used waiting time x as a continuous variable, allowing us to estimate how\n",
    "#each unit increase in waiting time affects the eruption duration. This is a more informative approach, as\n",
    "#it enables us to:\n",
    "\n",
    "#Predict Changes in Duration with Waiting Time: Each model provides a slope coefficient \n",
    "#beta1, which shows the effect of a one-unit increase in x on Y.\n",
    "\n",
    "#Capture Detailed Trends:\n",
    "#Model 1: Uses the entire dataset, giving a single intercept and slope. This approach confirms an overall\n",
    "#relationship between waiting time and duration but may miss nuances within different ranges of x.\n",
    "#Models 2 and 3: By splitting the data into short and long wait times and fitting separate lines, we can\n",
    "#observe if the relationship between waiting time and duration differs between short and long waits.\n",
    "#These models are likely better fits since they capture potential differences in behavior across waiting\n",
    "#time ranges.\n",
    "\n",
    "#Conclusion\n",
    "#Overall, the indicator variable model lacks the nuance of the continuous models, as it overlooks the actual\n",
    "#waiting time values in favor of an oversimplified categorical split. The continuous models, particularly those\n",
    "#that consider separate slopes for short and long wait times, offer richer insights by revealing how eruption\n",
    "#duration changes across different waiting times.\n",
    "\n",
    "#Your analysis provides a clear and effective comparison, demonstrating why continuous models provide more\n",
    "#detailed, actionable insights compared to a simplified indicator model. Great job!\n",
    "\n",
    "\n",
    "\n",
    "#Link: https://chatgpt.com/share/672d1b42-d6ec-800a-87b4-43a4e298de80\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98642aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6620c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
